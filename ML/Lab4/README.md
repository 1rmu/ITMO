# Наивный байесовский классификатор

Целями данной работы являлись:
* создать n-грамм;
* реализовать алгоритм наивного байесовского классификатора;
* настроить штраф ошибки с целью минимизации ошибки второго рода;
* проанализировать результаты.

*Набор данных*

В данной работе рассматривалась задача классификации электронных писем на два класса: спам (spam) и не спам (legit). Папка files содержит набор данных из писем, каждое из которых 
состоит из заголовка и текста сообщения. Набор данных разбит на 10 частей для перекрёстной проверки. В каждом письме слова закодированы в численном представлении.

*n-граммы*

Было реализовано преобразование, которое представляет письма в виде разреженного вектора признаков. Преобразование поддерживает n-граммы и учитывать как заголовок, так и содержание письма. 
Были применены различные параметры сглаживания alpha и n для n-грамм. У сглаживания alpha оптимальное значение вещественное и сильно близкое к 0. Параметр n был проверен среди значений n = 1, 2, 3.

Далее была обучена модель на предсказание класса письма, построена ROC-кривую для обученной модели, посчитана точность (accuracy), используя перекрестную проверку (k-fold, где k=10).

Был подобран штраф ошибки классификации $λ_{legit}$ такой, чтобы ни одно реальное (legit) сообщение не было классифицировано как спам. Штраф $λ_{spam}$ при этом был зафиксирован.
